{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup cell!!\n",
    "\n",
    "from canvasapi import Canvas\n",
    "from canvasapi.exceptions import BadRequest\n",
    "from datetime import datetime, time, timedelta\n",
    "from zoneinfo import ZoneInfo\n",
    "import ipywidgets as widgets\n",
    "import os.path\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import pandas as pd\n",
    "from thefuzz import fuzz, process\n",
    "\n",
    "# setup canvas connection\n",
    "tokenfile = open(\"canvas-token\", \"r\")\n",
    "token = tokenfile.readline()\n",
    "tokenfile.close()\n",
    "url = \"https://westminster.instructure.com/\"\n",
    "canvas = Canvas(url, token)\n",
    "\n",
    "# now i'ma hardcode the courses that people will ask for extensions in\n",
    "# this will avoid a number of (slow!) requests and filters.\n",
    "math202id = 3387585\n",
    "math202 = canvas.get_course(math202id)\n",
    "phys309id = 3388078\n",
    "phys309 = canvas.get_course(phys309id)\n",
    "coursedict = {\n",
    "    \"MATH 202\": math202,\n",
    "    \"PHYS 309\": phys309\n",
    "}\n",
    "\n",
    "# setup google connection\n",
    "# If modifying these scopes, delete the file google-token.json.\n",
    "scopes = ['https://www.googleapis.com/auth/spreadsheets.readonly']\n",
    "spreadsheet_id = '1mtc172TQIju8BwAiqwFS8Fwtz1I8ZN6Aw7SwfrVvdG0'\n",
    "data_range = 'A2:I' #include the A column for a unique timestamp\n",
    "\n",
    "creds = None\n",
    "# The file google-token.json stores the user's access and refresh tokens, and is\n",
    "# created automatically when the authorization flow completes for the first time.\n",
    "# I am .gitignoring both google-token.json and google-credentials.json.\n",
    "if os.path.exists('google-token.json'):\n",
    "    creds = Credentials.from_authorized_user_file('google-token.json', scopes)\n",
    "# If there are no (valid) credentials available, let the user log in.\n",
    "if not creds or not creds.valid:\n",
    "    if creds and creds.expired and creds.refresh_token:\n",
    "        creds.refresh(Request())\n",
    "    else:\n",
    "        flow = InstalledAppFlow.from_client_secrets_file(\n",
    "            'google-credentials.json', scopes)\n",
    "        creds = flow.run_local_server(port=0)\n",
    "    # Save the credentials for the next run\n",
    "    with open('google-token.json', 'w') as token:\n",
    "        token.write(creds.to_json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function: combine two PaginatedLists\n",
    "def combine_pl(pl1, pl2):\n",
    "    # iterate through both to build the elements lists\n",
    "    # this is because these objects are loaded lazily from the API\n",
    "    for item in pl1: pass\n",
    "    for item in pl2: pass\n",
    "    pl1._elements = pl1._elements + pl2._elements\n",
    "    return pl1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function: remove unpublished assignments\n",
    "def remove_unpublished(pl):\n",
    "    i = 0\n",
    "    dellist = []\n",
    "    for item in pl: \n",
    "        # keep track of not-published items\n",
    "        if not item.published: dellist.append(i)\n",
    "        i+=1\n",
    "    # remove assignments that aren't published\n",
    "    for i in sorted(dellist, reverse=True):\n",
    "        del pl._elements[i]\n",
    "    return pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will also pre-pull the assignment lists and do a little pre-filtering.\n",
    "# in math 202 specifically, I only care about problem sets and webwork\n",
    "# so let's grab those by groups\n",
    "for group in math202.get_assignment_groups(): \n",
    "    if group.name == \"Problem Sets\":\n",
    "        math202_assgs = math202.get_assignments_for_group(group.id)\n",
    "    elif group.name == \"Webwork\":\n",
    "        math202_webwork = math202.get_assignments_for_group(group.id)\n",
    "\n",
    "math202_assgs = remove_unpublished(combine_pl(math202_assgs, math202_webwork))\n",
    "\n",
    "phys309_assgs = remove_unpublished(phys309.get_assignments())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from spreadsheet\n",
    "try:\n",
    "    service = build('sheets', 'v4', credentials=creds)\n",
    "\n",
    "    # Call the Sheets API\n",
    "    sheet = service.spreadsheets()\n",
    "    result = sheet.values().get(spreadsheetId=spreadsheet_id,\n",
    "                                range=data_range).execute()\n",
    "    requests = result.get('values', [])\n",
    "    requestsdf = pd.DataFrame(requests, columns = [\"timestamp\", \"name\", \"email\", \"classname\", \n",
    "        \"assignment\", \"due date\", \"need1\", \"need2\", \"status\"])\n",
    "    # pull out the ones that I need to do something with\n",
    "    # tododf = requestsdf[pd.isna(requestsdf[\"status\"])]\n",
    "    tododf = requestsdf\n",
    "    \n",
    "\n",
    "    if not requests:\n",
    "        print('No data found.')\n",
    "except HttpError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function: build dropdown list of assignments (no fuzz)\n",
    "def find_assignment(assglist):\n",
    "    opts = [(\"Ignore this request\", None)]\n",
    "    for assg in assglist:\n",
    "        opts.append((assg.name, assg))\n",
    "    dropdown = widgets.Dropdown(\n",
    "        options=opts,\n",
    "        value=None,\n",
    "        description=\"Assignment: \",\n",
    "        disabled=False,\n",
    "    )\n",
    "    return dropdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEP -- utility function: fuzzy string match assignments\n",
    "# given a list of assignments and a string that a student wrote,\n",
    "# I'm going to create a dropdown of possibilities,\n",
    "# sorted by matching score,\n",
    "# and pre-populated with my best guess.\n",
    "# Currently I am deprecating this because it needs a little massaging.\n",
    "def fuzzy_find_assignment(assglist, inputstr):\n",
    "    namesdict = {}\n",
    "    for assg in assglist:\n",
    "        namesdict[assg.name]=assg\n",
    "    # calculate and sort by matching score: \n",
    "    fuzzresults = process.extract(inputstr, namesdict.keys())\n",
    "    opts = [] \n",
    "    for result in fuzzresults:\n",
    "        opts.append( (result[0], namesdict[result[0]]) )\n",
    "        # this is now a list of tuples like widgets.Dropdown wants.\n",
    "    dropdown = widgets.Dropdown(\n",
    "        options=opts,\n",
    "        # value=None, -- I'm going to prepopulate with my best guess.\n",
    "        description=\"Probably: \",\n",
    "        disabled=False,\n",
    "    )\n",
    "    return dropdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b12d85d5b14a068b29641a98c3200e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(VBox(children=(Label(value='Ted Scott'), HBox(children=(Label(value='Webwork 5.1-5.2'), Dropdownâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023ebacc4fe24516a57dfbefd47e5e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = widgets.Output()\n",
    "assignmentdict = {} #This is going to record what I click, and then we merge on timestamp.\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html\n",
    "\n",
    "def buttonclick(timestamp, dropdown):\n",
    "    assignmentdict[timestamp.value] = dropdown.value #oops\n",
    "    with output:\n",
    "        print(timestamp.value + \" \" + str(dropdown.value))\n",
    "\n",
    "def requestbox(request): \n",
    "    timestamp = widgets.Label(value=request[\"timestamp\"])\n",
    "    name = widgets.Label(value=request[\"name\"])\n",
    "    dropdown = find_assignment(math202_assgs)\n",
    "    button = widgets.Button(description = \"Looks good!\")\n",
    "    button.on_click(lambda b: buttonclick(timestamp, dropdown))\n",
    "\n",
    "    return widgets.VBox([\n",
    "        name,\n",
    "        widgets.HBox([widgets.Label(value=request[\"assignment\"]), \n",
    "                      dropdown]),\n",
    "        widgets.Label(value=request[\"due date\"]), \n",
    "        widgets.HTML(value=request[\"need1\"]),\n",
    "        widgets.HTML(value=request[\"need2\"]),\n",
    "        button\n",
    "        ])\n",
    "tab = widgets.Tab(children = tododf.apply(requestbox, axis=1).tolist())\n",
    "display(tab, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tododf\u001b[39m.\u001b[39mmerge(pd\u001b[39m.\u001b[39;49mDataFrame(assignmentdict), how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m, on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(tododf)\n",
      "File \u001b[1;32mc:\\Users\\orang\\Documents\\GitHub\\python-canvas-ddc\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:664\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    658\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[0;32m    659\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[0;32m    660\u001b[0m     )\n\u001b[0;32m    662\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    663\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 664\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[0;32m    665\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[0;32m    666\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\orang\\Documents\\GitHub\\python-canvas-ddc\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    490\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    491\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[1;32m--> 493\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[1;32mc:\\Users\\orang\\Documents\\GitHub\\python-canvas-ddc\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:118\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    116\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[0;32m    119\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\orang\\Documents\\GitHub\\python-canvas-ddc\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:656\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    653\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    655\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m indexes \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m raw_lengths:\n\u001b[1;32m--> 656\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIf using all scalar values, you must pass an index\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    658\u001b[0m \u001b[39melif\u001b[39;00m have_series:\n\u001b[0;32m    659\u001b[0m     index \u001b[39m=\u001b[39m union_indexes(indexes)\n",
      "\u001b[1;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "tododf.merge(pd.DataFrame(assignmentdict), how='left', on='timestamp')\n",
    "print(tododf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_override_dict(request): # request is a row of the google sheet\n",
    "    [timestamp, name, email, coursename, assignment, dd, need1, need2, status] = request\n",
    "\n",
    "    # find the correct course and lookup the student\n",
    "    course = coursedict[coursename]\n",
    "    try: student = course.get_users(search_term = email)[0]\n",
    "    except IndexError as err:\n",
    "        print(\"haha that email's wrong\")\n",
    "        return\n",
    "    \n",
    "    # parse the date correctly as a datetime object\n",
    "    # comes in from google forms as, e.g., \"1/30\"\n",
    "    duetime = timedelta(hours=23,minutes=59)\n",
    "    duedate = datetime.strptime(dd+\"/23\", '%m/%d/%y').replace(tzinfo=ZoneInfo(\"America/Denver\"))\n",
    "    duedate = duedate + duetime\n",
    "\n",
    "    return {\n",
    "        'student_ids': [student.id],\n",
    "        'due_at': duedate, \n",
    "        'all_day': True,\n",
    "        'all_day_date':  str(duedate.date())\n",
    "    }\n",
    "\n",
    "# a thought: should this actually be able to deal with multiple students requesting the same new due date for the same assignment?\n",
    "# that may be more headache than it is actually worth.\n",
    "# maybe what should happen is, I build individual override dicts,\n",
    "# and then before I fire off all the overrides,\n",
    "# a helper function checks *all* the override dicts I have built,\n",
    "# together with all existing overrides,\n",
    "# and collapses any that it can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so now what I am going to do is something like, \n",
    "# filter the requestsdf to only have the todo ones\n",
    "# then tododf.apply(build_override_dict, axis=1) \n",
    "\n",
    "# at some point we also need to do the assignment lookup\n",
    "# and finally commit all of those overrides to canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 5 (2799972295.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 11\u001b[1;36m\u001b[0m\n\u001b[1;33m    try:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'for' statement on line 5\n"
     ]
    }
   ],
   "source": [
    "# hey so this is 74% of the body of the write override function\n",
    "# i wanna write this as assg.write_override()\n",
    "def write_override(self, ovdict):\n",
    "    overrides = self.get_overrides()\n",
    "    for override in overrides:\n",
    "        # if there is some kind of intersection between the student ids in the existing overrides\n",
    "        # and the student id in the ovdict I am throwing you, \n",
    "        # then we should edit the existing override to remove the student\n",
    "        # and create a new override for the student\n",
    "\n",
    "    try:\n",
    "        self.create_override(assignment_override = dict)\n",
    "    except BadRequest: # I need to be a little more careful here.\n",
    "        # It's possible that the student is *one* of several students in an override.\n",
    "        # If so, I need to remove them from the old override\n",
    "        # and create them a new override\n",
    "        # or else add them to an existing override\n",
    "        # (otherwise it might fuck up other people's overrides!)\n",
    "        bad_override = next((ov for ov in test_assg.get_overrides() if 12261567 in ov.student_ids), None)\n",
    "        bad_override.edit(\n",
    "            assignment_override={\n",
    "                'student_ids': [12261567],\n",
    "                'due_at': datetime(2023,2,1,23,59,59,tzinfo=ZoneInfo('America/Denver')),\n",
    "                'all_day': True,\n",
    "                'all_day_date':  '2023-02-01'\n",
    "            }\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b28ad25eaab6a05f687d6a4dab7858dd95f20a0cea1c945d61ffde17fb6a74d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
